{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":353,"status":"ok","timestamp":1714421557040,"user":{"displayName":"Samara Miramontes","userId":"13244449749203433893"},"user_tz":420},"id":"JN_MbqGfdgyt","outputId":"82c1f182-803e-4993-e11e-4d9ca94ad1de"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2, 5, 5)\n"]}],"source":["import numpy as np\n","\n","# Define the reward matrix\n","rewards = np.array([\n","    [0, 0.2],\n","    [0, 0.2],\n","    [0, 0.2],\n","    [0, 0.2],\n","    [1, 0.2]\n","])\n","\n","# Define transition probabilities for a0 and a1\n","transition_a0 = np.array([\n","    [0, 0.8, 0.2, 0, 0],\n","    [0, 0, 0.8, 0.2, 0],\n","    [0, 0, 0.2, 0.8, 0],\n","    [0, 0, 0, 0, 1],\n","    [0, 0, 0, 0, 1]\n","])\n","\n","transition_a1 = np.array([\n","    [0.9, 0.1, 0, 0, 0],\n","    [0.9, 0.1, 0, 0, 0],\n","    [0.9, 0, 0.1, 0, 0],\n","    [0.9, 0, 0.1, 0, 0],\n","    [0.9, 0, 0.1, 0, 0]\n","])\n","\n","transition = np.array([transition_a0, transition_a1])\n","print(transition.shape)\n","\n","# Discount factor\n","gamma = 0.95\n","\n","# Number of states and actions\n","num_states = 5\n","num_actions = 2"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714421955921,"user":{"displayName":"Samara Miramontes","userId":"13244449749203433893"},"user_tz":420},"id":"ZqHBYedVfzeU","outputId":"4e025175-5dd8-4191-9925-ba08d9345889"},"outputs":[{"name":"stdout","output_type":"stream","text":["defaultdict(<class 'dict'>, {0: {0: [(1, 0.8), (2, 0.2)], 1: [(0, 0.9), (1, 0.1)]}, 1: {0: [(2, 0.8), (3, 0.2)], 1: [(0, 0.9), (1, 0.1)]}, 2: {0: [(2, 0.2), (3, 0.8)], 1: [(0, 0.9), (2, 0.1)]}, 3: {0: [(4, 1.0)], 1: [(0, 0.9), (2, 0.1)]}, 4: {0: [(4, 1.0)], 1: [(0, 0.9), (2, 0.1)]}})\n"]}],"source":["import collections\n","trans = collections.defaultdict(dict)\n","for state in range(num_states):\n","  for action in range(num_actions):\n","    trans[state][action] = []\n","    for nxt_state, prob in enumerate(transition[action, state, :]):\n","      if prob == 0:\n","        continue\n","      else:\n","        trans[state][action].append((nxt_state, prob))\n","\n","print(trans)"]},{"cell_type":"markdown","metadata":{"id":"MTO9jwcZIT2E"},"source":["#Optimal Q Values for all state-action pairs using dynamic programming:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1714421571082,"user":{"displayName":"Samara Miramontes","userId":"13244449749203433893"},"user_tz":420},"id":"p5gKa_Oxdgyz","outputId":"8f4d3b62-e67f-439a-b971-f5a30f3630c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Q-values from Value Iteration:\n"," [[16.42772839 15.87577877]\n"," [17.15864198 15.87577877]\n"," [17.82716049 15.93928802]\n"," [19.         15.93928802]\n"," [20.         15.93928802]]\n"]}],"source":["# Value Iteration Function\n","def value_iteration(rewards\n","                    , transition_a0\n","                    , transition_a1\n","                    , gamma\n","                    , num_states\n","                    , num_actions\n","                    , threshold=0.00000000001):\n","    Q = np.zeros((num_states, num_actions))\n","    delta = float('inf')\n","\n","    while delta > threshold:\n","        delta = 0\n","        for s in range(num_states):\n","            for a in range(num_actions):\n","                old_value = Q[s, a]\n","                if a == 0:\n","                    Q[s, a] = rewards[s, a] + gamma * np.sum(transition_a0[s, :] * np.max(Q, axis=1))\n","                else:\n","                    Q[s, a] = rewards[s, a] + gamma * np.sum(transition_a1[s, :] * np.max(Q, axis=1))\n","                delta = max(delta, abs(old_value - Q[s, a]))\n","\n","    return Q\n","\n","# Compute the Q-values using Value Iteration\n","Q_values = value_iteration(rewards, transition_a0, transition_a1, gamma, num_states, num_actions)\n","print(\"Q-values from Value Iteration:\\n\", Q_values)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hHe-k0JwItl4"},"source":["#Q learning with epsilon greedy exploration:"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26341,"status":"ok","timestamp":1714422893965,"user":{"displayName":"Samara Miramontes","userId":"13244449749203433893"},"user_tz":420},"id":"RaokXFPsdgy0","outputId":"5d5465ae-bf6b-4aaf-92a9-13454e726cff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode: 50, Q(s,a) = \n","[[16.40997478  7.10373645]\n"," [17.15107778  6.93260958]\n"," [17.8260686   7.58182233]\n"," [18.99991805  6.87460271]\n"," [19.99999089 15.89577355]]\n","Episode: 100, Q(s,a) = \n","[[16.42772839 12.79096806]\n"," [17.15864197 11.95274927]\n"," [17.82716049 13.63009513]\n"," [19.         13.25215336]\n"," [20.         15.93928802]]\n","Episode: 150, Q(s,a) = \n","[[16.4277284  14.68844722]\n"," [17.15864198 14.4398231 ]\n"," [17.82716049 15.21960247]\n"," [19.         15.01326868]\n"," [20.         15.93928802]]\n","Episode: 200, Q(s,a) = \n","[[16.4277284  15.47070154]\n"," [17.15864198 15.328612  ]\n"," [17.82716049 15.71499005]\n"," [19.         15.66757291]\n"," [20.         15.93928802]]\n","Episode: 250, Q(s,a) = \n","[[16.4277284  15.75450659]\n"," [17.15864198 15.63090893]\n"," [17.82716049 15.85635841]\n"," [19.         15.85374961]\n"," [20.         15.93928802]]\n","Episode: 300, Q(s,a) = \n","[[16.4277284  15.83760117]\n"," [17.15864198 15.78152933]\n"," [17.82716049 15.91771924]\n"," [19.         15.91493457]\n"," [20.         15.93928802]]\n","Episode: 350, Q(s,a) = \n","[[16.4277284  15.86490931]\n"," [17.15864198 15.83876597]\n"," [17.82716049 15.93436537]\n"," [19.         15.93130684]\n"," [20.         15.93928802]]\n","Episode: 400, Q(s,a) = \n","[[16.4277284  15.87258942]\n"," [17.15864198 15.85954431]\n"," [17.82716049 15.937642  ]\n"," [19.         15.93645342]\n"," [20.         15.93928802]]\n","Episode: 450, Q(s,a) = \n","[[16.4277284  15.87481429]\n"," [17.15864198 15.86920816]\n"," [17.82716049 15.93884681]\n"," [19.         15.93828128]\n"," [20.         15.93928802]]\n","Episode: 500, Q(s,a) = \n","[[16.4277284  15.8754464 ]\n"," [17.15864198 15.87346848]\n"," [17.82716049 15.93916614]\n"," [19.         15.93900426]\n"," [20.         15.93928802]]\n","Final Q-values:\n","[[16.4277284  15.8754464 ]\n"," [17.15864198 15.87346848]\n"," [17.82716049 15.93916614]\n"," [19.         15.93900426]\n"," [20.         15.93928802]]\n"]}],"source":["import numpy as np\n","\n","# Initialization\n","num_states = 5\n","num_actions = 2\n","Q = np.zeros((num_states, num_actions))  # Q-value matrix\n","lr = 0.01\n","epsilon = 0.1  # Initial exploration probability\n","min_epsilon = 0.001  # Minimum value of epsilon\n","epsilon_decay = 0.999  # Decay factor for epsilon\n","gamma = 0.95  # Discount factor\n","episodes = 500  # Total number of episodes\n","steps_per_episode = 1000  # Number of steps per episode\n","\n","# Q-Learning with Epsilon-Greedy Exploration\n","for epi in range(episodes):\n","    state = 0  # Always start from state s0\n","    for step in range(steps_per_episode):\n","        if np.random.rand() < epsilon:\n","            action = np.random.choice(num_actions)  # Explore: choose a random action\n","        else:\n","            action = np.argmax(Q[state])  # Exploit: choose the best action based on Q-values\n","\n","        # Simulate transition based on action\n","        tran = trans[state][action]\n","        reward = rewards[state][action]\n","\n","        # print(tran)\n","\n","        q_sa = reward\n","        for nxt_state, prob in tran:\n","          q_sa += prob * gamma * np.max(Q[nxt_state])\n","\n","        # Q-value update\n","        td_error = q_sa - Q[state][action]\n","        Q[state][action] += lr * td_error\n","\n","        # Transition to next state\n","        nxt_states = [nxt_state for nxt_state, prob in tran]\n","        nxt_probs = [prob for nxt_state, prob in tran]\n","\n","        state = np.random.choice(nxt_states, 1, p=nxt_probs)[0]\n","\n","    # Epsilon decay\n","    # epsilon = max(min_epsilon, epsilon * epsilon_decay)\n","    if (epi + 1) % 50 == 0:\n","        print(f'Episode: {epi + 1}, Q(s,a) = \\n{Q}')\n","\n","# Print the final Q-values\n","print(\"Final Q-values:\")\n","print(Q)"]},{"cell_type":"markdown","metadata":{"id":"uThZN6blI1OS"},"source":["#Compare your results obtained by the two methods:\n","\n","The Q values for all state-action pairs using dynamic programming results were:\n","```\n","Q-values from Value Iteration:\n"," [[16.42772839 15.87577877]\n"," [17.15864198 15.87577877]\n"," [17.82716049 15.93928802]\n"," [19.         15.93928802]\n"," [20.         15.93928802]]\n","```\n","Meanwhile Q learning with epsilon greedy exploration, the values were:\n","```\n","Final Q-values:\n","[[16.4277284  15.8754464 ]\n"," [17.15864198 15.87346848]\n"," [17.82716049 15.93916614]\n"," [19.         15.93900426]\n"," [20.         15.93928802]]\n","```\n","As you can tell, they are incredibly close to each other.\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
